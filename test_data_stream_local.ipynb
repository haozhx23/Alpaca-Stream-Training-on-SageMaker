{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4164bcc7-86bf-444a-8340-3c84df644c3e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install datasets\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11f1284d-8e74-45b1-9de5-fd087c4b7bc1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import logging\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, Optional, Sequence\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "# import utils\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import Trainer\n",
    "\n",
    "############################\n",
    "from datetime import datetime\n",
    "import os\n",
    "# import deepspeed\n",
    "import json, socket\n",
    "import argparse\n",
    "from datasets import load_dataset\n",
    "import datasets\n",
    "from functools import partial\n",
    "from typing import Dict, Optional, Sequence, Any\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a3f1b09-8b38-437a-8f50-b54e11c2229e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fileslist = {'train': '1_AM_wiki_00'}\n",
    "# train_data = load_dataset('json', data_files=fileslist, split='train')\n",
    "train_data = load_dataset('json', data_files=fileslist, split='train', streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b454882-55ba-487d-a29b-b624690f8f39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "IGNORE_INDEX = -100\n",
    "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
    "DEFAULT_EOS_TOKEN = \"</s>\"\n",
    "DEFAULT_BOS_TOKEN = \"<s>\"\n",
    "DEFAULT_UNK_TOKEN = \"<unk>\"\n",
    "\n",
    "def _tokenize_fn(strings: Sequence[str], tokenizer: transformers.PreTrainedTokenizer) -> Dict:\n",
    "    \"\"\"Tokenize a list of strings.\"\"\"\n",
    "    tokenized_list = [\n",
    "        tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"longest\",\n",
    "            max_length=tokenizer.model_max_length,\n",
    "            truncation=True,\n",
    "        )\n",
    "        for text in strings\n",
    "    ]\n",
    "    input_ids = labels = [tokenized.input_ids[0] for tokenized in tokenized_list]\n",
    "    input_ids_lens = labels_lens = [\n",
    "        tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item() for tokenized in tokenized_list\n",
    "    ]\n",
    "    return dict(\n",
    "        input_ids=input_ids,\n",
    "        labels=labels,\n",
    "        input_ids_lens=input_ids_lens,\n",
    "        labels_lens=labels_lens,\n",
    "    )\n",
    "\n",
    "def preprocess(\n",
    "        sources: Sequence[str],\n",
    "        targets: Sequence[str],\n",
    "        tokenizer: transformers.PreTrainedTokenizer,\n",
    ") -> Dict:\n",
    "    \"\"\"Preprocess the data by tokenizing.\"\"\"\n",
    "    examples = [s + t for s, t in zip(sources, targets)]\n",
    "    examples_tokenized, sources_tokenized = [_tokenize_fn(strings, tokenizer) for strings in (examples, sources)]\n",
    "    input_ids = examples_tokenized[\"input_ids\"]\n",
    "    labels = copy.deepcopy(input_ids)\n",
    "    for label, source_len in zip(labels, sources_tokenized[\"input_ids_lens\"]):\n",
    "        label[:source_len] = IGNORE_INDEX\n",
    "    return dict(input_ids=input_ids, labels=labels)\n",
    "\n",
    "def tokenize_wiki_batch(\n",
    "        tokenizer: transformers.PreTrainedTokenizer,\n",
    "        list_data_dict: Dict[str, Any],\n",
    "):\n",
    "\n",
    "    txtinput = \"\\n{title}\\n\\n\\t\\t{text}\\n\\n\"\n",
    "\n",
    "    # example = list_data_dict\n",
    "    # print('---DDD---',list_data_dict['text'])\n",
    "    \n",
    "    titlelist = list_data_dict['title']\n",
    "    textlist = list_data_dict['text']\n",
    "    batchlen = len(list_data_dict['text'])\n",
    "\n",
    "    sources = [\n",
    "        f\"{tokenizer.bos_token}\" for _ in titlelist\n",
    "    ]\n",
    "    # targets = [txtinput.format_map(example) if example.get(\"title\", \"\") != \"\" else example.get(\"text\", \"\")\n",
    "    #           for example in list_data_dict\n",
    "    #           ]\n",
    "    \n",
    "    \n",
    "    targets = [txtinput.format_map({'title':titlelist[i],'text':textlist[i]}) if textlist[i] != \"\" else \"\\n\"\n",
    "              for i in range(batchlen)\n",
    "              ]\n",
    "        \n",
    "\n",
    "    # logging.warning(\"Tokenizing inputs... This may take some time...\")\n",
    "    print('---DDD---',len(sources),len(targets))\n",
    "    data_dict = preprocess(sources, targets, tokenizer)\n",
    "    input_ids = data_dict[\"input_ids\"]\n",
    "    labels = data_dict[\"labels\"]\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"labels\": labels,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc38b8e8-f208-420a-bdb2-0f48682a5526",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    'TheBloke/Llama-2-7B-fp16',\n",
    "    cache_dir='',\n",
    "    model_max_length=2048,\n",
    "    padding_side=\"right\",\n",
    "    use_fast=False,\n",
    "    legacy=False ############ p5 test\n",
    ")\n",
    "\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "# buffer_size not allow for batch mode\n",
    "train_dataset = train_data.shuffle(10000).map(\n",
    "    partial(\n",
    "        # tokenize_prompt_gen,\n",
    "        tokenize_wiki_batch,\n",
    "        tokenizer\n",
    "    ),\n",
    "    remove_columns=['id','url','title','text'],\n",
    "    batched=True\n",
    ")\n",
    "\n",
    "# @dataclass\n",
    "# class DataCollatorForSupervisedDataset(object):\n",
    "#     \"\"\"Collate examples for supervised fine-tuning.\"\"\"\n",
    "\n",
    "#     tokenizer: transformers.PreTrainedTokenizer\n",
    "\n",
    "#     def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
    "#         input_ids, labels = tuple([instance[key] for instance in instances] for key in (\"input_ids\", \"labels\"))\n",
    "\n",
    "#         input_ids = torch.Tensor([torch.nn.utils.rnn.pad_sequence(\n",
    "#             input_id, batch_first=True, padding_value=self.tokenizer.pad_token_id) for input_id in input_ids])\n",
    "#         labels = [torch.nn.utils.rnn.pad_sequence(label, batch_first=True, padding_value=IGNORE_INDEX) for label in labels]\n",
    "#         attention_mask = [input_id.ne(self.tokenizer.pad_token_id) for input_id in input_ids]\n",
    "\n",
    "#         # input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "#         #     input_ids[0], batch_first=True, padding_value=self.tokenizer.pad_token_id)\n",
    "#         # labels = torch.nn.utils.rnn.pad_sequence(labels[0], batch_first=True, padding_value=IGNORE_INDEX)\n",
    "#         # attention_mask = input_ids.ne(self.tokenizer.pad_token_id)\n",
    "\n",
    "#         return dict(\n",
    "#             input_ids=input_ids,\n",
    "#             labels=labels,\n",
    "#             attention_mask=attention_mask,\n",
    "#         )\n",
    "\n",
    "    \n",
    "# data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n",
    "# # data_module = dict(train_dataset=train_data, eval_dataset=None, data_collator=data_collator)\n",
    "# data_module = dict(train_dataset=train_data.with_format(\"torch\"), eval_dataset=None, data_collator=data_collator)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ac4608-cfb9-4db6-993a-907bf8279327",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1da9a89-7a8b-42f2-af43-2b5250e3d309",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---DDD--- 790 790\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([    1,     1,    13, 30313, 30319,   313, 31046, 30732, 29897,    13,\n",
       "            13,    12,    12, 30313, 30319,   313, 31046, 30732, 29897,    13,\n",
       "            13, 30866, 30313, 30319, 30843, 30419, 30409, 30214,   232,   146,\n",
       "           139, 31685, 30866, 31203, 30313, 30577, 30319, 30843, 31391, 30866,\n",
       "           233,   139,   183,   236,   190,   148,   232,   187,   192, 30210,\n",
       "         31203, 30313, 30843, 30214, 30392, 29896, 29929, 29953, 29946, 30470,\n",
       "         31419, 31107, 30594, 31480, 31424, 31195, 30888, 31349, 31046, 30613,\n",
       "           236,   158,   186, 30728, 30064, 31530, 31168, 31107, 31141,   234,\n",
       "           190,   155, 31072, 30210, 30287,   232,   188,   136, 31046, 30732,\n",
       "         30267,   232,   177,   134, 30392, 30287,   232,   188,   136, 30688,\n",
       "         31046, 31551, 30214,   234,   190,   155, 31072, 30210, 30392, 30287,\n",
       "         30502,   233,   139,   183,   235,   148,   154,   232,   159,   137,\n",
       "           236,   164,   185,   234,   164,   175,   234,   167,   191,   232,\n",
       "           187,   192, 30330,   234,   172,   194,   235,   148,   154, 31143,\n",
       "         30257,   235,   164,   166, 30210, 31203, 30319, 31305, 31133, 30214,\n",
       "           235,   134,   143, 31495, 30392, 31784, 30503, 30257, 30581, 30267,\n",
       "         31203, 30319, 30210,   235,   135,   187, 31407, 30287, 30502,   233,\n",
       "           133,   175, 30816, 30210, 30986,   235,   142,   188, 30801,   233,\n",
       "           143,   164,   231,   192,   146, 30743, 30214,   231,   192,   137,\n",
       "         31221,   231,   190,   144, 31594,   235,   142,   188, 30801, 30210,\n",
       "         31993,   234,   191,   155, 30429,   234,   173,   168, 31568,   235,\n",
       "           148,   154,   235,   170,   133,   231,   191,   154, 30267, 31203,\n",
       "         30319, 30210, 31651,   235,   138,   133,   235,   178,   164,   232,\n",
       "           191,   133, 30533, 31331, 30822,   232,   191,   178,   233,   141,\n",
       "           155, 30267, 30732, 30573, 30287,   232,   188,   136, 30417, 30548,\n",
       "         30210, 31480, 31424, 31195, 30888, 31349, 30732, 31399, 30214,   232,\n",
       "           177,   134, 30544, 31424, 30505, 30743, 30822, 30793, 30210,   235,\n",
       "           177,   187, 30923, 31679, 31619, 30732, 31399, 30847, 30866,   232,\n",
       "           159,   166, 30329, 30843, 30330, 30866,   232,   132,   186, 30408,\n",
       "           233,   187,   187,   233,   139,   146, 30843, 30330, 30866, 30856,\n",
       "         30868, 30313, 30486, 30843, 30275, 30267,    13,    13, 30406, 31530,\n",
       "         31168, 31107, 31141, 30688,   232,   186,   180, 30210, 31852, 30805,\n",
       "         31639, 30383,    13,    13,    13]),\n",
       " 'labels': tensor([ -100,  -100,    13, 30313, 30319,   313, 31046, 30732, 29897,    13,\n",
       "            13,    12,    12, 30313, 30319,   313, 31046, 30732, 29897,    13,\n",
       "            13, 30866, 30313, 30319, 30843, 30419, 30409, 30214,   232,   146,\n",
       "           139, 31685, 30866, 31203, 30313, 30577, 30319, 30843, 31391, 30866,\n",
       "           233,   139,   183,   236,   190,   148,   232,   187,   192, 30210,\n",
       "         31203, 30313, 30843, 30214, 30392, 29896, 29929, 29953, 29946, 30470,\n",
       "         31419, 31107, 30594, 31480, 31424, 31195, 30888, 31349, 31046, 30613,\n",
       "           236,   158,   186, 30728, 30064, 31530, 31168, 31107, 31141,   234,\n",
       "           190,   155, 31072, 30210, 30287,   232,   188,   136, 31046, 30732,\n",
       "         30267,   232,   177,   134, 30392, 30287,   232,   188,   136, 30688,\n",
       "         31046, 31551, 30214,   234,   190,   155, 31072, 30210, 30392, 30287,\n",
       "         30502,   233,   139,   183,   235,   148,   154,   232,   159,   137,\n",
       "           236,   164,   185,   234,   164,   175,   234,   167,   191,   232,\n",
       "           187,   192, 30330,   234,   172,   194,   235,   148,   154, 31143,\n",
       "         30257,   235,   164,   166, 30210, 31203, 30319, 31305, 31133, 30214,\n",
       "           235,   134,   143, 31495, 30392, 31784, 30503, 30257, 30581, 30267,\n",
       "         31203, 30319, 30210,   235,   135,   187, 31407, 30287, 30502,   233,\n",
       "           133,   175, 30816, 30210, 30986,   235,   142,   188, 30801,   233,\n",
       "           143,   164,   231,   192,   146, 30743, 30214,   231,   192,   137,\n",
       "         31221,   231,   190,   144, 31594,   235,   142,   188, 30801, 30210,\n",
       "         31993,   234,   191,   155, 30429,   234,   173,   168, 31568,   235,\n",
       "           148,   154,   235,   170,   133,   231,   191,   154, 30267, 31203,\n",
       "         30319, 30210, 31651,   235,   138,   133,   235,   178,   164,   232,\n",
       "           191,   133, 30533, 31331, 30822,   232,   191,   178,   233,   141,\n",
       "           155, 30267, 30732, 30573, 30287,   232,   188,   136, 30417, 30548,\n",
       "         30210, 31480, 31424, 31195, 30888, 31349, 30732, 31399, 30214,   232,\n",
       "           177,   134, 30544, 31424, 30505, 30743, 30822, 30793, 30210,   235,\n",
       "           177,   187, 30923, 31679, 31619, 30732, 31399, 30847, 30866,   232,\n",
       "           159,   166, 30329, 30843, 30330, 30866,   232,   132,   186, 30408,\n",
       "           233,   187,   187,   233,   139,   146, 30843, 30330, 30866, 30856,\n",
       "         30868, 30313, 30486, 30843, 30275, 30267,    13,    13, 30406, 31530,\n",
       "         31168, 31107, 31141, 30688,   232,   186,   180, 30210, 31852, 30805,\n",
       "         31639, 30383,    13,    13,    13])}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d32b8429-c225-4e20-8915-21fba0c36713",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.24.4'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy\n",
    "numpy.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9df2bd-c040-4190-94e1-e23b31d08f08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
